[Unit]
Description=BenchAI LLM Servers (Optimized Stack)
After=network.target docker.service
Wants=docker.service

[Service]
Type=forking
User=user
WorkingDirectory=/home/user/benchai
Environment="PATH=/home/user/.local/bin:/usr/local/bin:/usr/bin:/bin"
ExecStartPre=/bin/bash -c 'pkill -9 -f llama-server || true'
ExecStartPre=/bin/sleep 2

# Start Qwen2.5-Coder-14B on GPU (port 8093)
ExecStart=/bin/bash -c '/home/user/llama.cpp/build/bin/llama-server \
    -m /media/data/llm-models/qwen2.5-coder-14b-instruct-q4_k_m.gguf \
    --host 127.0.0.1 --port 8093 \
    -ngl 99 -c 4096 -np 1 -t 8 \
    --flash-attn on -b 256 \
    --log-disable &'

# Start Phi-3 Mini on CPU (port 8091)
ExecStartPost=/bin/bash -c 'sleep 15 && /home/user/llama.cpp/build/bin/llama-server \
    -m /home/user/llama.cpp/models/phi-3-mini-4k-instruct.Q4_K_M.gguf \
    --host 127.0.0.1 --port 8091 \
    -ngl 0 -c 4096 -t 6 -b 512 --mlock \
    --log-disable &'

# Start Qwen2.5-7B on CPU (port 8092)
ExecStartPost=/bin/bash -c 'sleep 8 && /home/user/llama.cpp/build/bin/llama-server \
    -m /home/user/llama.cpp/models/qwen2.5-7b-instruct.Q5_K_M.gguf \
    --host 127.0.0.1 --port 8092 \
    -ngl 0 -c 4096 -t 6 -b 512 --mlock \
    --log-disable &'

ExecStop=/bin/bash -c 'pkill -9 -f llama-server || true'
RemainAfterExit=yes
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
